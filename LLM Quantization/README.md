# LLM Quantization techniques

This is a survey of the available LLM Quantization techniques. I try to cover

- The intuition
- Python code sample
- What the performance is like

## What is LLM Quantization?

Quantization is a technique that aims to reduce the model's size by reducing the granularity of the model's weights. For example, by storing the model weights as 64, we could convert them to a lower resolution, say int8. This reduces the model's memory footprint, inference time, but at the cost of accuracy. Quantization can be applied naively to all model weights, or only to specific parts (layers, blocks, consecutive layers) of the model. Model parts to be quantized, and what's the resolution are determined through, you guessed it, some algorithms with some policies!

There are different ways to group quantization techniques:

- When to quantize:
  - During-training: Done during training, or fine tuning. This is considered expensive
  - Post-training (aka one-shot) methods: do not require retraining the model. In this camp, there is a group of techniques, called Large-model quantization, which address directly the challenge of handling large models.

- What to quantize:
  - Weight quantization: Quantize the weights of the model
  - Activation quantization: Quantize the activations of the model (the outputs of the layers). This take into account the data distribution of the activations during inference.

- Symmetry
  - Symmetric quantization: The range of the original floating-point values are mapped to a range around 0 of the quantized space. This does not utilize the full range of the quantized space.
    - Signed: For example, -128 to 127 for int 8
    - Unsigned: For example, 0 to 255 for int 8. More suitable for one-tailed distributions, such as those produced by the ReLU activation function.
  - Asymmetric quantization: The range of the original floating-point values are mapped to the same range of the quantized space. This fully utilises the quantized range.

## Why does this work?

In terms of efficiency:

- Neural networks are commonly trained using floating point numbers for weights and activation. At inference time, these numbers need to be moved from memory to the processing units for computation. This requires the processing elements (and the accumulators) to support floating point numbers which involves more resource and power consumption.
- The cost of digital arithmetic scales linearly to quadratically with the number of bits used to represent numbers, so using less bits (through quantization) makes the computation cheaper.
- Hardware can handle fixed point numbers better than floating point numbers.

Why reducing the precision of the model doesn't harm its accuracy? Here are a few theories:

- The models are overparameterized, and the model can be compressed without losing much accuracy.
- Generalist models (e.g., GPT4) don't use all of their parameters for a downstream tasks. Those parameters can be essentially thrown away through quantization, without harming the task's performance.

## Building blocks of quantization techniques

These are some small quantization techniques which serve as the components for other more sophisticated ones.

### Layer-wise quantization

Try to replace all the weights of a layer with a set of weights of a lower resolution. Given a small number of data points, the weights in the new layer should seek to minimize the squared error between the output generated by the original layer and that of the new layer.

### Optimal brain quantization

For each row in the weight matrix, try to quantize one weight at a time, and adjust the remaining weights to compensate for the reconstruction errors (same concept in layer-wise quantization) introduced by quantization. It tries to prioritise weights that result in the least additional quantization error. Repeat this until all the weights are quantized. Although this is embarassingly parallel, the computational complexity is O(n_rows * n_cols**3) for a weight matrix of n_rows x n_cols makes this extremely expensive.

[This paper](https://arxiv.org/pdf/2210.17323) argues that the improvement of "trying to prioritise weights that result in the least additional quantization error" over a random order is really small, especially on large and heavy parametrized layers. Most likely because eventually, all weights need to be quantized. This process only defers the quantization of weights that are harder to quantize without introducing large error to the end.

## Best practices

- It's common to apply asymetric quantization to the activations and symmetric quantization to the weights. This is because at inference time, the activations are more likely to be one-tailed (ReLU activation function), and symmetric quantization is more suitable for two-tailed distributions.

## Quantization techniques

### GPTQ

Paper: <https://arxiv.org/pdf/2106.08295>
Original repo: <https://github.com/IST-DASLab/gptq>

Largely inspired by [OBQ](#optimal-brain-quantization). It implements some improvements:

- Instead of quantizing one weight at a time like in OBQ, it quantizes a whole column in the weight matrix at once. -> Time complexity from `O(n_rows * n_cols ** 3)` to `O(max{nrows * n_cols ** 2, n_cols**3})`. This is based on their experience that the order of which weight to quantize doesn't matter (much). It then add some GPU optimization and mathematical optimization to make the process faster and numerically more stable.
- Lazy batch-updates: When quantizing the weights, because OBQ updates one weight at a time, it needs to round the weight to the quantized value at each step. But since GPTQ does this by each column, it can defer the rounding until later. Also, since each column is independent, it can batch the updates, and perform them in parallel later. Although this doesn't reduce the amount of compute needed, it fully utilises the memory of the GPU (compared to updating 1 weight at a time).
- Updating the hessian matrix (a step required in OBQ) is numerically unstable. This is because this steps need to be performed repeatedly which can accumulate errors. It gets worse for larger models due to the large number of weights to be updated. GPTQ uses Cholesky decomposition to precompute the necessary information from the hessian matrix. Combined with mild dampening (adding a small constant to the diagonal elements), this is more stable and faster.

Experimental results show that this is much faster than the popular quantization techniques (at the time), while still maintain the same level of accuracy for generation tasks. They also found  that larger models are easier to quantize. Memory footprint is reduced by 4-5 times (OPT-175B being an example), coupled with 3-4 time speed up in inference time.

<details><summary>Code snippet</summary>

Taken from <https://huggingface.co/neuralmagic/Phi-3-mini-128k-instruct-quantized.w8a8>

```python
from transformers import AutoTokenizer
from datasets import Dataset
from llmcompressor.transformers import SparseAutoModelForCausalLM, oneshot
from llmcompressor.modifiers.quantization import GPTQModifier
import random

model_id = "microsoft/Phi-3-mini-128k-instruct"

num_samples = 256
max_seq_len = 8192

tokenizer = AutoTokenizer.from_pretrained(model_id)

max_token_id = len(tokenizer.get_vocab()) - 1
input_ids = [[random.randint(0, max_token_id) for _ in range(max_seq_len)] for _ in range(num_samples)]
attention_mask = num_samples * [max_seq_len * [1]]
ds = Dataset.from_dict({"input_ids": input_ids, "attention_mask": attention_mask})

recipe = GPTQModifier(
  targets="Linear",
  scheme="W8A8",
  ignore=["lm_head"],
  dampening_frac=0.01,
)

model = SparseAutoModelForCausalLM.from_pretrained(
  model_id,
  device_map="auto",
  trust_remote_code=True,
)

oneshot(
  model=model,
  dataset=ds,
  recipe=recipe,
  max_seq_length=max_seq_len,
  num_calibration_samples=num_samples,
)

model.save_pretrained("Phi-3-mini-128k-instruct-quantized.w8a8")
```

</details>
